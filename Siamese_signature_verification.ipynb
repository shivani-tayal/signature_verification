{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signature verification using siamese network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import cv2\n",
    "import time\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate, Dropout\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.core import Lambda, Flatten, Dense\n",
    "from keras.initializers import glorot_uniform\n",
    "\n",
    "from keras.engine.topology import Layer\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the Dataset:\n",
    "\n",
    "\n",
    "The BHSig260 signature dataset contains the signatures of 260 persons, among them 100 were signed in Bengali and 160 are signed in Hindi. \n",
    "\n",
    "For each of the signers, 24 genuine and 30 forged signatures are available. This results in 100 × 24 = 2, 400 genuine and 100 × 30 = 3, 000 forged signatures in Bengali, and 160 × 24 = 3, 840 genuine and 160×30 = 4, 800 forged signatures in Hindi.\n",
    "\n",
    "In this task we are considering only Hindi singatures for easeness. \n",
    "\n",
    "\n",
    "**Paper Link:**  https://arxiv.org/pdf/1707.02131.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Considering only Hindi signatures from the dataset.\n",
    "\n",
    "###### Dataset has Hindi and Gujrati signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./BHSig260/Hindi/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of all directories and sort them\n",
    "dir_list = next(os.walk(path))[1]\n",
    "dir_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['001', '002', '003']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List sample folders\n",
    "dir_list[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each person segregate the genuine signatures from the forged signatures\n",
    "# Genuine signatures are stored in the list \"orig_groups\"\n",
    "# Forged signatures are stored in the list \"forged_groups\"\n",
    "orig_groups, forg_groups = [], []\n",
    "for directory in dir_list:\n",
    "    images = os.listdir(path+directory)\n",
    "    images.sort()\n",
    "    images = [path+directory+'/'+x for x in images]\n",
    "    forg_groups.append(images[:30]) # First 30 signatures in each folder are forrged\n",
    "    orig_groups.append(images[30:]) # Next 24 signatures are genuine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160, 160)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quick check to confirm we have data of all the 160 individuals\n",
    "len(orig_groups), len(forg_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_lengths = [len(x) for x in orig_groups]\n",
    "forg_lengths = [len(x) for x in forg_groups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24]\n"
     ]
    }
   ],
   "source": [
    "# Quick check to confirm that there are 24 Genuine signatures for each individual\n",
    "print(orig_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30]\n"
     ]
    }
   ],
   "source": [
    "# Quick check to confirm that there are 30 Forged signatures for each individual\n",
    "print(forg_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-Validation-Test Split\n",
    "* Signatures of 120 people are used for training\n",
    "* Signatures of 20 people are used for validation\n",
    "* Signatures of 20 people are used for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_train, orig_val, orig_test = orig_groups[:120], orig_groups[120:140], orig_groups[140:]\n",
    "forg_train, forg_val, forg_test = forg_groups[:120], forg_groups[120:140], forg_groups[140:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete unnecessary variables\n",
    "del orig_groups, forg_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the images will be converted to the same size before processing\n",
    "img_h, img_w = 155, 220"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_sample_signature():\n",
    "    '''Function to randomly select a signature from train set and\n",
    "    print two genuine copies and one forged copy'''\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize = (10, 10))\n",
    "    k = np.random.randint(len(orig_train))\n",
    "    orig_img_names = random.sample(orig_train[k], 2)\n",
    "    forg_img_name = random.sample(forg_train[k], 1)\n",
    "    orig_img1 = cv2.imread(orig_img_names[0], 0)\n",
    "    orig_img2 = cv2.imread(orig_img_names[1], 0)\n",
    "    forg_img = cv2.imread(forg_img_name[0], 0)\n",
    "    orig_img1 = cv2.resize(orig_img1, (img_w, img_h))\n",
    "    orig_img2 = cv2.resize(orig_img2, (img_w, img_h))\n",
    "    forg_img = cv2.resize(forg_img, (img_w, img_h))\n",
    "\n",
    "    ax1.imshow(orig_img1, cmap = 'gray')\n",
    "    ax2.imshow(orig_img2, cmap = 'gray')\n",
    "    ax3.imshow(forg_img, cmap = 'gray')\n",
    "\n",
    "    ax1.set_title('Genuine Copy')\n",
    "    ax1.axis('off')\n",
    "    ax2.set_title('Genuine Copy')\n",
    "    ax2.axis('off')\n",
    "    ax3.set_title('Forged Copy')\n",
    "    ax3.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_sample_signature()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(orig_groups, forg_groups, batch_size = 32):\n",
    "    '''Function to generate a batch of data with batch_size number of data points\n",
    "    Half of the data points will be Genuine-Genuine pairs and half will be Genuine-Forged pairs'''\n",
    "    while True:\n",
    "        orig_pairs = []\n",
    "        forg_pairs = []\n",
    "        gen_gen_labels = []\n",
    "        gen_for_labels = []\n",
    "        all_pairs = []\n",
    "        all_labels = []\n",
    "        \n",
    "        # Here we create pairs of Genuine-Genuine image names and Genuine-Forged image names\n",
    "        # For every person we have 24 genuine signatures, hence we have \n",
    "        # 24 choose 2 = 276 Genuine-Genuine image pairs for one person.\n",
    "        # To make Genuine-Forged pairs, we pair every Genuine signature of a person\n",
    "        # with 12 randomly sampled Forged signatures of the same person.\n",
    "        # Thus we make 24 * 12 = 276 Genuine-Forged image pairs for one person.\n",
    "        # In all we have 120 person's data in the training data.\n",
    "        # Total no. of Genuine-Genuine pairs = 120 * 276 = 33120\n",
    "        # Total number of Genuine-Forged pairs = 120 * 276 = 33120\n",
    "        # Total no. of data points = 33120 + 33120 = 66240\n",
    "        for orig, forg in zip(orig_groups, forg_groups):\n",
    "            orig_pairs.extend(list(itertools.combinations(orig, 2)))\n",
    "            for i in range(len(forg)):\n",
    "                forg_pairs.extend(list(itertools.product(orig[i:i+1], random.sample(forg, 12))))\n",
    "        \n",
    "        # Label for Genuine-Genuine pairs is 1\n",
    "        # Label for Genuine-Forged pairs is 0\n",
    "        gen_gen_labels = [1]*len(orig_pairs)\n",
    "        gen_for_labels = [0]*len(forg_pairs)\n",
    "        \n",
    "        # Concatenate all the pairs together along with their labels and shuffle them\n",
    "        all_pairs = orig_pairs + forg_pairs\n",
    "        all_labels = gen_gen_labels + gen_for_labels\n",
    "        del orig_pairs, forg_pairs, gen_gen_labels, gen_for_labels\n",
    "        all_pairs, all_labels = shuffle(all_pairs, all_labels)\n",
    "        \n",
    "        # Note the lists above contain only the image names and\n",
    "        # actual images are loaded and yielded below in batches\n",
    "        # Below we prepare a batch of data points and yield the batch\n",
    "        # In each batch we load \"batch_size\" number of image pairs\n",
    "        # These images are then removed from the original set so that\n",
    "        # they are not added again in the next batch.\n",
    "            \n",
    "        k = 0\n",
    "        pairs=[np.zeros((batch_size, img_h, img_w, 1)) for i in range(2)]\n",
    "        targets=np.zeros((batch_size,))\n",
    "        for ix, pair in enumerate(all_pairs):\n",
    "            img1 = cv2.imread(pair[0], 0)\n",
    "            img2 = cv2.imread(pair[1], 0)\n",
    "            img1 = cv2.resize(img1, (img_w, img_h))\n",
    "            img2 = cv2.resize(img2, (img_w, img_h))\n",
    "            img1 = np.array(img1, dtype = np.float64)\n",
    "            img2 = np.array(img2, dtype = np.float64)\n",
    "            img1 /= 255\n",
    "            img2 /= 255\n",
    "            img1 = img1[..., np.newaxis]\n",
    "            img2 = img2[..., np.newaxis]\n",
    "            pairs[0][k, :, :, :] = img1\n",
    "            pairs[1][k, :, :, :] = img2\n",
    "            targets[k] = all_labels[ix]\n",
    "            k += 1\n",
    "            if k == batch_size:\n",
    "                yield pairs, targets\n",
    "                k = 0\n",
    "                pairs=[np.zeros((batch_size, img_h, img_w, 1)) for i in range(2)]\n",
    "                targets=np.zeros((batch_size,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(vects):\n",
    "    '''Compute Euclidean Distance between two vectors'''\n",
    "    x, y = vects\n",
    "    return K.sqrt(K.sum(K.square(x - y), axis=1, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eucl_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(y_true, y_pred):\n",
    "    '''Contrastive loss from Hadsell-et-al.'06\n",
    "    Source: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    \n",
    "    Explanation:\n",
    "    When ytrue is 1, that means the sample are duplicates of each other, \n",
    "    so the Euclidean distance (ypred) between their outputs must be minimized.\n",
    "    So the loss is taken as the square of that Euclidean distance itself - K.square(y_pred).\n",
    "    When ytrue is 0, i.e. the samples are not duplicates, then the Euclidean distance \n",
    "    between them must be maximized, at least to the margin. So the loss to be minimized\n",
    "    is the difference of the margin and the Euclidean distance - (margin - y_pred).\n",
    "    If the Euclidean distance (ypred) is already greater than the margin, \n",
    "    then nothing is to be learned, so the loss is made to be zero in \n",
    "    that case by saying K.maximum(margin - y_pred, 0).\n",
    "    '''\n",
    "    margin = 1\n",
    "    return K.mean(y_true * K.square(y_pred) + (1 - y_true) * K.square(K.maximum(margin - y_pred, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_base_network_signet(input_shape):\n",
    "    '''Base Siamese Network'''\n",
    "    \n",
    "    seq = Sequential()\n",
    "    seq.add(Conv2D(96, kernel_size=(11, 11), activation='relu', name='conv1_1', strides=4, input_shape= input_shape))\n",
    "    seq.add(BatchNormalization())\n",
    "    seq.add(MaxPooling2D((3,3), strides=(2, 2)))    \n",
    "    seq.add(ZeroPadding2D((2, 2)))\n",
    "    \n",
    "    seq.add(Conv2D(256, kernel_size=(5, 5), activation='relu', name='conv2_1'))\n",
    "    seq.add(BatchNormalization())\n",
    "    seq.add(MaxPooling2D((3,3), strides=(2, 2)))\n",
    "    seq.add(Dropout(0.3))\n",
    "    seq.add(ZeroPadding2D((1, 1)))\n",
    "    \n",
    "    seq.add(Conv2D(384, kernel_size=(3, 3), activation='relu', name='conv3_1'))\n",
    "    seq.add(ZeroPadding2D((1, 1)))\n",
    "    \n",
    "    seq.add(Conv2D(256, kernel_size=(3, 3), activation='relu', name='conv3_2'))    \n",
    "    seq.add(MaxPooling2D((3,3), strides=(2, 2)))\n",
    "    seq.add(Dropout(0.3))\n",
    "    seq.add(Flatten(name='flatten'))\n",
    "    seq.add(Dense(1024, activation='relu'))\n",
    "    seq.add(Dropout(0.5))\n",
    "    \n",
    "    seq.add(Dense(128, activation='relu')) # softmax changed to relu\n",
    "    \n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape=(img_h, img_w, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/rohit.raj1/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/rohit.raj1/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/rohit.raj1/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/rohit.raj1/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/rohit.raj1/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/rohit.raj1/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/rohit.raj1/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/rohit.raj1/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/rohit.raj1/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/rohit.raj1/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/rohit.raj1/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/rohit.raj1/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/rohit.raj1/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rohit.raj1/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:16: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"la...)`\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "# network definition\n",
    "base_network = create_base_network_signet(input_shape)\n",
    "\n",
    "input_a = Input(shape=(input_shape))\n",
    "input_b = Input(shape=(input_shape))\n",
    "\n",
    "# because we re-use the same instance `base_network`,\n",
    "# the weights of the network\n",
    "# will be shared across the two branches\n",
    "processed_a = base_network(input_a)\n",
    "processed_b = base_network(input_b)\n",
    "\n",
    "# Compute the Euclidean distance between the two vectors in the latent space\n",
    "distance = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([processed_a, processed_b])\n",
    "\n",
    "model = Model(input=[input_a, input_b], output=distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(69120, 11520, 11520)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_sz = 128\n",
    "num_train_samples = 276*120 + 300*120\n",
    "num_val_samples = num_test_samples = 276*20 + 300*20\n",
    "num_train_samples, num_val_samples, num_test_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/rohit.raj1/opt/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# compile model using RMSProp Optimizer and Contrastive loss function defined above\n",
    "rms = RMSprop(lr=1e-4)\n",
    "model.compile(loss=contrastive_loss, optimizer=rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 155, 220, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 155, 220, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 128)          6462272     input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1)            0           sequential_1[1][0]               \n",
      "                                                                 sequential_1[2][0]               \n",
      "==================================================================================================\n",
      "Total params: 6,462,272\n",
      "Trainable params: 6,461,568\n",
      "Non-trainable params: 704\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Keras Callbacks, save the model after every epoch\n",
    "# Reduce the learning rate by a factor of 0.1 if the validation loss does not improve for 5 epochs\n",
    "# Stop the training using early stopping if the validation loss does not improve for 12 epochs\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=12, verbose=1),\n",
    "    ReduceLROnPlateau(factor=0.1, patience=5, min_lr=0.000001, verbose=1),\n",
    "    ModelCheckpoint('./signet-bhsig260-{epoch:03d}.h5', verbose=1, save_weights_only=True)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.fit_generator(generate_batch(orig_train, forg_train, batch_sz),\n",
    "                              steps_per_epoch = num_train_samples//batch_sz,\n",
    "                              epochs = 20,\n",
    "                              #epochs=20\n",
    "                              validation_data = generate_batch(orig_val, forg_val, batch_sz),\n",
    "                              validation_steps = num_val_samples//batch_sz,\n",
    "                              callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy_roc(predictions, labels):\n",
    "    '''Compute ROC accuracy with a range of thresholds on distances.\n",
    "    '''\n",
    "    dmax = np.max(predictions)\n",
    "    dmin = np.min(predictions)\n",
    "    nsame = np.sum(labels == 1)\n",
    "    ndiff = np.sum(labels == 0)\n",
    "   \n",
    "    step = 0.01\n",
    "    max_acc = 0\n",
    "    best_thresh = -1\n",
    "   \n",
    "    for d in np.arange(dmin, dmax+step, step):\n",
    "        idx1 = predictions.ravel() <= d\n",
    "        idx2 = predictions.ravel() > d\n",
    "       \n",
    "        tpr = float(np.sum(labels[idx1] == 1)) / nsame       \n",
    "        tnr = float(np.sum(labels[idx2] == 0)) / ndiff\n",
    "        acc = 0.5 * (tpr + tnr)       \n",
    "#       print ('ROC', acc, tpr, tnr)\n",
    "       \n",
    "        if (acc > max_acc):\n",
    "            max_acc, best_thresh = acc, d\n",
    "           \n",
    "    return max_acc, best_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('./signet-bhsig260-002.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen = generate_batch(orig_test, forg_test, 1)\n",
    "pred, tr_y = [], []\n",
    "for i in range(num_test_samples):\n",
    "    (img1, img2), label = next(test_gen)\n",
    "    tr_y.append(label)\n",
    "    pred.append(model.predict([img1, img2])[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_acc, threshold = compute_accuracy_roc(np.array(pred), np.array(tr_y))\n",
    "tr_acc, threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy = 79.37% and Threshold = 0.32\n",
    "Thus if the differnce score is less than 0.32, we predict the test image as Genuine and if the difference score is greater than 0.32, we predict it to be as forged\n",
    "\n",
    "#### Below we see some sample results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_score():\n",
    "    '''Predict distance score and classify test images as Genuine or Forged'''\n",
    "    test_point, test_label = next(test_gen)\n",
    "    img1, img2 = test_point[0], test_point[1]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (10, 10))\n",
    "    ax1.imshow(np.squeeze(img1), cmap='gray')\n",
    "    ax2.imshow(np.squeeze(img2), cmap='gray')\n",
    "    ax1.set_title('Genuine')\n",
    "    if test_label == 1:\n",
    "        ax2.set_title('Genuine')\n",
    "    else:\n",
    "        ax2.set_title('Forged')\n",
    "    ax1.axis('off')\n",
    "    ax2.axis('off')\n",
    "    plt.show()\n",
    "    result = model.predict([img1, img2])\n",
    "    diff = result[0][0]\n",
    "    print(\"Difference Score = \", diff)\n",
    "    if diff > threshold:\n",
    "        print(\"Its a Forged Signature\")\n",
    "    else:\n",
    "        print(\"Its a Genuine Signature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: The first image is always Genuine. Score prediction and classification is done for the second image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refrence litrature for the notebook\n",
    "\n",
    "# http://researchers.lille.inria.fr/abellet/talks/metric_learning_tutorial_CIL.pdf\n",
    "# https://arxiv.org/abs/1306.6709"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
